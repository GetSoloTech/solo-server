from txtai import RAG
from litgpt import LLM
from rich.console import Console
import os

import nltk
nltk.download(['punkt', 'punkt_tab'])

from txtai.pipeline import Textractor

from txtai import Embeddings


# Create Textractor
textractor = Textractor()
text = textractor("solodocs/solo-server/solo_server/commands/solodocs/Bug Report Template.docx")
print(text)
     

def stream(path):
  for f in sorted(os.listdir(path)):
    fpath = os.path.join(path, f)

    # Only accept documents
    if f.endswith(("docx", "xlsx", "pdf")):
      print(f"Indexing {fpath}")
      for paragraph in textractor(fpath):
        yield paragraph

# Document text extraction, split into paragraphs

# Vector Database
embeddings = Embeddings(content=True)
embeddings.index(stream("solodocs"))

console = Console()

def recommend_based_on_docs(query: str):
    """
    Generate a recommendation answer (with citations) based on the documentation context using a RAG pipeline.

    The function uses the following documentation context:

    "As we discussed before, this is important when dealing with large volumes of data. Not all of the data can be added to a LLM prompt. Additionally, having only the most relevant context helps the LLM generate higher quality answers.

    Citations for LLMs
    A healthy level of skepticism should be applied to answers generated by AI. We're far from the day where we can blindly trust answers from an AI model.

    txtai has a couple approaches for generating citations. The basic approach is to take the answer and search the vector database for the closest match.

    for x in embeddings.search(result):
      print(x['text'])
         
    E5-base-v2
    Image Captions BLIP
    Labels - Zero Shot BART-Large-MNLI
    Model Guide
    |Component |Model(s)|Date Added|
    |---|---|---|
    |Embeddings |all-MiniLM-L6-v2|2022-04-15|
    |Image Captions |BLIP|2022-03-17|
    |Labels - Zero Shot |BART-Large-MNLI|2022-01-01|
    |Large Language Model (LLM) |Mistral 7B OpenOrca|2023-10-01|
    |Summarization |DistilBART|2021-02-22|
    |Text-to-Speech |ESPnet JETS|2022-08-01|
    |Transcription |Whisper|2022-08-01|
    |Translation |OPUS Model Series|2021-04-06|

    While the basic approach above works in this case, txtai has a more robust pipeline to handle citations and references.
    The RAG pipeline is defined below. A RAG pipeline works in the same way as a LLM + Vector Search pipeline, except it has special logic for generating citations. 
    This pipeline takes the answers and compares it to the context passed to the LLM to determine the most likely reference."

    The function prints the generated answer and the corresponding citation.
    """
    # Documentation context to guide the answer
    docs_context = (
        "As we discussed before, this is important when dealing with large volumes of data. Not all of the data can be added to a LLM prompt. "
        "Additionally, having only the most relevant context helps the LLM generate higher quality answers.\n\n"
        "Citations for LLMs:\n"
        "A healthy level of skepticism should be applied to answers generated by AI. We're far from the day where we can blindly trust answers from an AI model.\n\n"
        "txtai has a couple approaches for generating citations. The basic approach is to take the answer and search the vector database for the closest match.\n\n"
        "for x in embeddings.search(result):\n  print(x['text'])\n\n"
        "E5-base-v2\n"
        "Image Captions BLIP\n"
        "Labels - Zero Shot BART-Large-MNLI\n\n"
        "Model Guide\n"
        "|Component |Model(s)|Date Added|\n"
        "|---|---|---|\n"
        "|Embeddings |all-MiniLM-L6-v2|2022-04-15|\n"
        "|Image Captions |BLIP|2022-03-17|\n"
        "|Labels - Zero Shot |BART-Large-MNLI|2022-01-01|\n"
        "|Large Language Model (LLM) |Mistral 7B OpenOrca|2023-10-01|\n"
        "|Summarization |DistilBART|2021-02-22|\n"
        "|Text-to-Speech |ESPnet JETS|2022-08-01|\n"
        "|Transcription |Whisper|2022-08-01|\n"
        "|Translation |OPUS Model Series|2021-04-06|\n\n"
        "While the basic approach above works in this case, txtai has a more robust pipeline to handle citations and references.\n\n"
        "The RAG pipeline is defined below. A RAG pipeline works in the same way as a LLM + Vector Search pipeline, except it has special logic for generating citations. "
        "This pipeline takes the answers and compares it to the context passed to the LLM to determine the most likely reference."
    )
    
    # Create a prompt that injects the documentation context
    def prompt_with_context(question: str):
        return [{
            "query": question,
            "question": f"""
Answer the following question using only the context below. Only include information specifically discussed.

question: {question}
context:
{docs_context}
"""
        }]
    
    # Create the LLM instance with a system prompt template.
    llm = LLM("TheBloke/Mistral-7B-OpenOrca-AWQ")
    
    # Create the RAG instance using txtai; the output mode "reference" will provide a reference id.
    rag = RAG(embeddings, llm, output="reference")
    
    # Query the RAG pipeline with the prompt that includes the docs context.
    result = rag(prompt_with_context(query), maxlength=4096, pad_token_id=32000)[0]
    
    console.print("ANSWER:", style="bold cyan")
    console.print(result["answer"], style="white")
    
    # Retrieve and print citation text using the reference from the result.
    citation = embeddings.search(
        "select id, text from txtai where id = :id", 
        limit=1, 
        parameters={"id": result["reference"]}
    )
    console.print("CITATION:", style="bold cyan")
    console.print(citation, style="white")

# Example usage:
if __name__ == "__main__":
    # Test the function with a sample recommendation query.
    test_query = "recommend how can I optimize model performance based on the docs provided?"
    recommend_based_on_docs(test_query)
